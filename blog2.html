<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Research Notes: Protein-Nanoparticle Binding | Shahmir Aziz</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <style>
        .blog-post {
            max-width: 700px;
            margin: 40px auto;
            padding: 20px;
            line-height: 1.8;
        }
        .blog-title {
            font-size: 28px;
            font-weight: 700;
            margin-bottom: 10px;
            color: #1a1a1a;
        }
        .blog-date {
            color: #666;
            font-size: 14px;
            margin-bottom: 30px;
        }
        .blog-content {
            font-size: 15px;
            color: #3a3a3a;
        }
        .blog-content h3 {
            font-size: 20px;
            margin: 30px 0 15px 0;
            color: #1a1a1a;
        }
        pre {
            background: #f5f5f5;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            font-size: 13px;
            line-height: 1.5;
            border-left: 3px solid #0d7a3e;
        }
        .equation {
            text-align: center;
            margin: 20px 0;
            font-style: italic;
        }
        .back-link {
            margin-bottom: 30px;
        }
        .note {
            background: #fffbf0;
            padding: 12px;
            margin: 20px 0;
            border-left: 3px solid #f0ad4e;
            font-size: 14px;
        }
        .update {
            background: #f0f8ff;
            padding: 12px;
            margin: 20px 0;
            border-left: 3px solid #5bc0de;
            font-size: 14px;
        }
    </style>
</head>
<body>
    <div class="blog-post">
        <div class="back-link">
            <a href="index.html">← Back to main</a>
        </div>

        <h1 class="blog-title">Research Notes: Transformer Networks for Protein-Nanoparticle Binding Prediction</h1>
        <div class="blog-date">Week 11 Update - November 2024</div>

        <div class="blog-content">
            <p>
                <strong>Current accuracy:</strong> R²=0.79 on test set (up from 0.31 baseline)<br>
                <strong>Inference time:</strong> 14ms per protein-particle pair<br>
                <strong>Major issue:</strong> Model fails catastrophically on glycosylated proteins
            </p>

            <h3>Why This Matters</h3>
            <p>
                Direct application to my lipid vesicle work - every nanoparticle that enters blood gets coated
                with proteins within seconds. This "corona" determines everything - where the particle goes,
                how long it circulates, whether it triggers immune response. For my asymmetric vesicles,
                the corona formation is even more complex due to curvature variations.
                Current methods (docking simulations) take 48 hours per protein. We have 3000+ plasma proteins.
                That's 4 months of compute for one particle design. Completely impractical.
            </p>

            <h3>The Approach</h3>
            <p>
                Forget sequences. Proteins don't bind via their amino acid sequence - they bind via 3D surface properties.
                Built a geometric transformer that operates directly on protein surface patches and nanoparticle geometry.
            </p>

            <div class="note">
                <strong>Key realization (Week 3):</strong> Standard transformers break rotational equivariance.
                If you rotate the protein, predictions change. That's physically wrong. Switched to SE(3)-equivariant
                attention. Immediately improved generalization.
            </div>

            <h3>Data Pipeline Hell</h3>
            <p>
                Spent 6 weeks just building the data pipeline. Three completely different data sources:
            </p>

            <ul>
                <li><strong>SPR measurements:</strong> Clean binding affinities but only for 47 proteins × 284 particles</li>
                <li><strong>MD simulations:</strong> Rich dynamics but computationally generated, not real</li>
                <li><strong>Mass spec proteomics:</strong> Real corona compositions but no individual affinities</li>
            </ul>

            <p>
                Had to write custom loss function that handles missing data and different measurement types:
            </p>

<pre>
def frankenstein_loss(pred, batch):
    loss = 0

    if batch.has_spr:
        # Direct supervision on binding free energy
        loss += mse(pred.dG, batch.dG_spr)

    if batch.has_proteomics:
        # Corona composition via softmax of affinities
        comp_pred = softmax(-pred.dG / kT)
        loss += kl_divergence(comp_pred, batch.mass_spec_abundance)

    if batch.has_md:
        # Contact probability from simulation
        contact_pred = sigmoid(-pred.dG / kT)
        loss += bce(contact_pred, batch.md_contacts)

    return loss
</pre>

            <div class="update">
                <strong>Update (Week 8):</strong> Multi-task learning is working! Model learned to extract
                useful features from all three data types. Proteomics data especially helpful for learning
                competitive binding effects.
            </div>

            <h3>Architecture Details</h3>
            <p>
                Current model has 31M parameters. Tried larger (100M) but no improvement.
                The bottleneck is data, not model capacity.
            </p>

<pre>
# Core architecture (simplified)
class BindingPredictor(nn.Module):
    def __init__(self):
        # Protein encoder: operates on surface patches
        self.protein_encoder = GeometricTransformer(
            hidden_dim=256,
            num_heads=8,
            num_layers=6
        )

        # Nanoparticle encoder: point cloud of surface
        self.nano_encoder = PointTransformer(
            hidden_dim=256,
            num_layers=4
        )

        # Cross-attention for binding interface
        self.interaction = CrossAttention(
            hidden_dim=256,
            num_heads=8,
            num_layers=4
        )
</pre>

            <h3>What's Working</h3>

            <p><strong>1. Surface patch representation</strong></p>
            <p>
                Instead of encoding entire protein, extract local surface patches (radius 15Å).
                Each patch: geometry + electrostatics + hydrophobicity. This captures binding sites naturally.
            </p>

            <p><strong>2. Augmentation via protein homologs</strong></p>
            <p>
                Found that training on homologous proteins improves generalization. If model sees mouse albumin
                binding gold nanoparticles, it better predicts human albumin binding. Increased training data 3x.
            </p>

            <div class="note">
                <strong>Unexpected finding:</strong> Model attention maps highlight known binding sites without
                supervision. For albumin, highest attention on Sudlow sites. For fibrinogen, focuses on
                D-domain. This suggests model learned meaningful biochemistry.
            </div>

            <h3>What's Broken</h3>

            <p><strong>1. Glycosylation disaster</strong></p>
            <p>
                Model completely fails on glycosylated proteins. Predictions off by 10-20 kJ/mol.
                Problem: training data mostly on E. coli expressed proteins (no glycosylation).
                Human proteins are heavily glycosylated. Major issue for translation.
            </p>

            <p><strong>2. Size extrapolation</strong></p>
            <p>
                Trained on 10-100nm particles. Tested on 200nm: predictions nonsense.
                Model learned size-specific features rather than general principles.
                Need to retrain with wider size range but limited experimental data.
            </p>

            <div class="update">
                <strong>Week 10 attempt:</strong> Tried to fix glycosylation by adding synthetic glycan trees
                to protein structures. Made predictions worse. Realized: glycans are flexible, single
                structure isn't representative. Need ensemble approach or different representation.
            </div>

            <h3>Experimental Validation</h3>
            <p>
                Collaborated with Robinson lab to test predictions. Made 12 custom particles with
                designed surface chemistries (including 3 based on my asymmetric vesicle designs). Results:
            </p>

            <ul>
                <li>8/12 particles: predictions within 2 kJ/mol (excellent)</li>
                <li>3/12 particles: off by 5-8 kJ/mol (okay)</li>
                <li>1/12 particles: completely wrong (predicted strong binding, observed none)</li>
            </ul>

            <p>
                The failure case had PEG mushroom configuration. Model never saw this during training.
                Classic out-of-distribution failure. Similar to issues we faced with PEG-lipids in the
                SOMA device coating - surface presentation matters enormously.
            </p>

            <h3>Current Focus</h3>

            <p>
                Working on uncertainty quantification. Model needs to know when it doesn't know.
                Implementing deep ensembles and testing on OOD detection. If model flags glycosylated
                proteins as uncertain, that's progress.
            </p>

<pre>
# Uncertainty via deep ensembles
predictions = []
for model in ensemble:
    predictions.append(model(protein, nanoparticle))

mean_pred = torch.mean(predictions)
uncertainty = torch.std(predictions)

# Flag high uncertainty for manual review
if uncertainty > threshold:
    return "Use physics-based simulation"
</pre>

            <h3>Next Month</h3>

            <ul>
                <li>Fix glycosylation (trying AlphaFold-multimer for glycan modeling)</li>
                <li>Add temperature dependence (currently assumes 37°C)</li>
                <li>Validate on in vivo data from mouse studies</li>
            </ul>

            <h3>Notes for Paper</h3>

            <p>
                Need to emphasize this is screening tool, not replacement for detailed simulation.
                Best use case: narrow 10^6 candidates to 100 for experimental testing.
            </p>

            <p>
                Reviewers will ask about negative data (particles that don't bind anything).
                We have very little - experiments only report positive binding. Major limitation.
            </p>

            <div class="note">
                <strong>Remember:</strong> Include ablation study showing that geometric features
                essential. Sequence-only baseline gets R²=0.19. Structure matters.
            </div>

            <p style="margin-top: 40px; font-style: italic;">
                Model checkpoint and cleaned data coming after publication. Too much unpublished
                collaborator data to release now. Code is honestly a mess - mixture of PyTorch,
                JAX for equivariance, and C++ for surface extraction. Needs major refactoring.
            </p>

            <p style="font-style: italic;">
                If you're working on protein-surface interactions, let's talk. Happy to test model
                on your system if you share binding data.
            </p>
        </div>
    </div>
</body>
</html>